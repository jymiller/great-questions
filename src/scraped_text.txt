I didn’t expect to have to update my views on the state-of-the-art in AI so soon after writing about Google’s Gemini Advanced, the first real competitor to GPT-4, but there have been two big leaps in Large Language Models this week with real practical implications.
The first has to do with memory: there is a new version of Google’s Gemini (right after the release of the previous one!), which has a context window of over a million tokens. The context window is the information that the AI can have in memory at one time, and most chatbots have been frustratingly limited, holding a couple dozen pages, at most. This is why it is very hard to use ChatGPT to write long programs or documents; it starts to forget the start of the project as its context window fills up.
But now Gemini 1.5 can hold something like 750,000 words in memory, with near-perfect recall. I fed it all my published academic work prior to 2022 — over 1,000 pages of PDFs spread across 20 papers and books — and Gemini was able to summarize the themes in my work and quote accurately from among the papers. There were no major hallucinations, only minor errors where it attributed a correct quote to the wrong PDF file, or mixed up the order of two phrases in a document. You can see how the advent of massive context windows gives AI superhuman recall and new use cases. If I asked a researcher to read through all my papers and summarize major themes, including illustrative quotes, it would take days. The AI did it in less than a minute. And Google has announced that context windows will soon reach 10 million tokens, or nearly 17,000 pages.
The second big advance is speed. You may have been frustrated by the relatively slow speed of ChatGPT, but one AI company, Groq (no relation to Elon Musk’s Grok), has developed hardware that gives almost instantaneous responses from GPT-3.5 class models, bridging the gap between question and answer in the blink of an eye. You can try it yourself. This shows that AI need not always involve waiting for replies.
Speed and memory are both vital to making AIs more usable and powerful in the real world. Imagine feeding AI hundreds of pages of instructions on how to do something, and then having it quickly do exactly that. In an experiment, I gave the AI a 352-page rulebook for an obscure game, and it was able to make sense of the scattered documentation and actually figure out how to correctly play. Plus, Google demonstrated exactly this capability in their Gemini 1.5 documentation. Researchers gave the five hundred or so available pages of reference material on a language with 200 speakers (and so with no real online presence) to Gemini and to a human translator. They found that the AI was able to learn the language about as well as the human could, from the same documentation, despite the fact that the AI itself was only about as smart as GPT-4. Together, rapid answers and massive context windows suggest that, even without smarter AIs (and those are coming soon), we will see large leaps in AI capabilities continue for the near future.
With this rapidly approaching future in mind, I taught the students in my classes this semester how to build prompts and distribute them as GPTs (a lot of those lessons I taught can be found in this post on how to create structured prompts and this post on creating GPTs). Most of my undergraduate and MBA students had no programming experience, but that was not a problem - building a GPT is more like providing instructions to a person than coding a machine. After those lessons, I gave my students an assignment titled “An Impossible Thing”: Build a GPT that will get you a job by showing a potential employer that you are a prompt engineer. It should automate a task in a job you want to do.
One of the things I emphasized to them is that no one really knows what current LLMs are capable of, since most of the tests conducted by AI companies focus mostly on coding and testing benchmarks, not real-world applications. Since my students come from many industries and countries, they had a tremendous diversity of potential use cases. By the time they started building GPTs for their specific needs, whether private equity deal memos or suggesting a perfect wedding ring to their fiancé, they became the world experts in using AI for their specific field. Let me share a few highlights (with permission):
Arunavha Chanda made a GPT that helps engineers and managers develop a common language around performance reviews. Oyinda Alliyu built a GPT to automate the process of creating social media posts from company “thought leadership” reports. Dragosh Castravet created a tool that allows search funds to more easily reach out to potential partners. Shanicee McKoy prototyped a GPT that helps real estate investors analyze rent rolls and deal memorandums. Hari Joy crafted a GPT that takes potential acquirers through a due diligence process. Stephen Serrao built a GPT that helps government officials understand economic development reports. Yuval Luxenburg automated the process of creating customer journeys and finding potential pain points with products. And Monica Phang developed a GPT that builds character sheets for roleplaying games. There were over a 120 other GPTs as well, many more than I can mention here.
I also asked students for a reflection on what they learned as a result of this process. An almost universal belief among the students, whether they were in aviation or consulting or banking or nonprofits, was that AI was going to have a big impact on the future of their industry very soon. Even though many saw the limitations of today’s tools, they also got a sense of where the future was heading. Despite this conviction, they also tended to think that the leaders and executives of the organizations they were joining did not yet see the full significance of AI and what it would mean for their industry.
So how can leaders start to think about the rapidly advancing nature of AI? The first thing they should do is use it. No amount of reading and research can substitute for spending 10 hours or so with a frontier model, learning what it can do. After getting familiar, companies should think about the following four questions:
What useful thing you do is no longer valuable? AI doesn’t do everything well, but it does some things very well. For many organizations, AI is fully capable of automating a task that used to be an important part of your organizational identity or strategy. AI comes up with more creative ideas than most people, so your company’s special brainstorming techniques may no longer be a big benefit. AI can provide great user journeys and personas, so your old product management approach is no longer a differentiator. Getting a sense of what AI can do now, and where it is heading, will allow you to have a realistic view of what might soon be delegated to an LLM.
What impossible thing can you do now? The flip side of the first question is that you now can do things that were impossible before. What does having an infinite number of interns for every employee get you? How does giving everyone a data analyst, marketer, and advisor change what is possible? You can look at some of the GPTs my students created as inspiration.
What can you move to a wider market or democratize? Prior to AI, companies were often advised to put their effort into servicing their most profitable customers, but AI has greatly changed the equation. Services and approaches that were once expensive to customize have become cheap. Prior to AI, strategy consulting firms would only work for giant clients for large fees, but now they may be able to offer effective advising to a much wider range of businesses at lower costs. Custom tutoring and mentoring, once available only to the rich, may be widely democratized.
What can you move upmarket or personalize? At the same time, your organization’s capabilities have increased. If you were once a small marketing firm, you can use AI to punch above your weight and offer services to elite clients that were once only available from much larger firms. With giant context windows and fast answers, every customer may be able to have a personal AI agent who knows their preferences and previous interactions with the company and communicates with them according to their preferences. Figure out the most exciting thing you can do, and see if you can make it happen.
Misguided companies will see any increase in performance from AI as an excuse to lay off staff, keeping their output the same. More forward-thinking firms will take advantage of these new capabilities to both improve the lives of their employees and expand their own capabilities. This is an area where leaders have agency over the future of AI and work. A lot depends on getting it right, and fast, because it is possible we are just getting started.
Many skeptics about the impact of AI are focused on the flaws that LLMs have today: hallucinations, short context windows, slow answers, and so on. These are legitimate concerns, and, if AI advancement were to stop, they might prove to be huge issues in the utility of AI. But AI is advancing rapidly, and some of these concerns may soon vanish, even if others (like hallucinations) are not completely solved.
What that means is that it is fine to be focused on today, building working AI applications and prompts that take into account the limits of present AIs… but there is also a lot of value in building ambitious applications that go past what LLMs can do now. You want to build some applications that almost, but not quite, work. I suspect better LLM “brains” are coming soon, in the form of GPT-5 and Gemini 2.0 and many others. When they do, you can swap them into the almost-but-not-quite-working applications for a fast start. This is similar to the philosophy of the big AI labs, which build ambitious solutions (OpenAI's GPT agents, Google's connections to Gmail) which will benefit when the next version of their core LLMs are released.
So don’t just build for what is possible today, but what is possible in six months, if this pace of change continues. At this point, I think things are unlikely to slow down in the near future, and focusing on where things are heading, rather than where they are, prepares you for a world of continuing change.
Leave a comment
My book on living and working with AI, Co-Intelligence, is coming out April 2. I am recording the audio book this week! You can pre-order it (including the audio version) here.
We’re headed for huge layoffs and a democratized market that sucks the profit out of most work. While individual forward-thinking companies may be able to move past other companies stuck in their ways, the overall market will not necessarily grow much in the sense of selling more clothing or more cars or more productivity software. There will be new markets created, for example on the consumer side, tutoring or AI friends etc, but I have a feeling they will be winner-take-all composed of a few oligopoly players or just one dominant player in each segment (if not rolled up across segments by google/openAI/Microsoft).
More generally, when something becomes easy, it doesn’t become more lucrative, it becomes less lucrative. Back when building websites was hard, or sourcing from China was hard, you could build a profitable business with a big moat. Now with Shopify or Alibaba or whatever, you have hundreds of thousands of entrants competing on who can undercut the most. It’s like an economics 101 class with perfect competition competing away all the producer surplus. AI will make white collar work of all kinds “easy”, and we will see the same result. The model providers (google/OpenAI/Microsoft/who-knows-mistral) will make all the money while millions of model users compete against each other
Great news about massive context windows and decreased latency. 
Yet you end on a critical note. Not to wear out Wayne Gretsky's advice  - but now is the time for us to skate to where the puck will be, not to where it is.
No posts
Ready for more?
